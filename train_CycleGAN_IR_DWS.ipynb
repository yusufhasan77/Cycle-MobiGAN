{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "00c6186b-e07b-4850-b67a-61034a1947db",
   "metadata": {},
   "source": [
    "# CycleGAN_IR_DWS"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "2938f037-e0a7-4bf9-a7b6-0c56b3455ee0",
   "metadata": {},
   "source": [
    "## Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a0a92419-072f-4c05-9eef-556eaf3391e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the necessary libraries\n",
    "# Import pytorch for creating neural networks\n",
    "import torch\n",
    "# Import nn module for creating neural networks\n",
    "import torch.nn as nn\n",
    "# import dataloader for loading data\n",
    "from torch.utils.data import DataLoader\n",
    "# Import transforms\n",
    "from torchvision import transforms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0ef48875-91b5-4216-abf5-d23b2c9d3c64",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import tqdm\n",
    "from tqdm import tqdm\n",
    "# Import torch summary to get a summary of the model\n",
    "#from torchsummary import summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0b89f21d-7d15-4be3-ac2f-51b1b8187e82",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the functions for training the models\n",
    "# Import the discriminators\n",
    "from Models.Discriminators import *\n",
    "# Import the generators\n",
    "from Models.Generators import *\n",
    "# Import the loss functions\n",
    "from Models.Loss_functions import disc_loss, gen_loss_complete\n",
    "\n",
    "# Import the function to load the data\n",
    "from Data.Dataset import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "030ff0a2-1805-434f-b244-aa63f30be362",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the visualization function\n",
    "from Utils.visualize_images import visualize_images"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "b1db36ed-91ba-4e47-9293-4fa50c70a29c",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Model training parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "71aff5b3-5545-42a0-b87f-7640adf6af0c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n"
     ]
    }
   ],
   "source": [
    "# Set the device\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "# Display the device\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "afca9bee-2be4-47d5-824a-69c6ff445ee0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the model training parameters\n",
    "# Set the number of epochs\n",
    "NUM_EPOCHS = 201\n",
    "# Set the training batch size\n",
    "BATCH_SIZE = 1\n",
    "# Set the learning rate to be used for the optimizers\n",
    "LEARNING_RATE = 0.0002\n",
    "# The size of the images after being loaded\n",
    "LOAD_SHAPE = 286\n",
    "# The size of the images after being cropped\n",
    "CROPPED_SHAPE = 256\n",
    "# Set seed\n",
    "torch.manual_seed(7)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "86706330-f921-490d-b78d-3172566113e0",
   "metadata": {},
   "source": [
    "## Loading the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f44a0cd6-1907-4009-861f-c30dc63a9575",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a transformation to be applied to images\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize(LOAD_SHAPE),\n",
    "    transforms.RandomHorizontalFlip(),\n",
    "    transforms.RandomCrop(CROPPED_SHAPE),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=(0.5, 0.5, 0.5), std=(0.5, 0.5, 0.5))\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "1125d48f-a526-416d-9dbf-acfaa1504eac",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the dataset\n",
    "dataset = LoadDataset(\"Data/apple2orange\", transform  = transform, mode=\"train\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "ea5e4165-0bbb-471f-9364-07bcc7100c21",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a dataloader\n",
    "dataloader = DataLoader(dataset, batch_size = BATCH_SIZE, shuffle=True)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "2271a021-b20e-4a8d-af7e-4d5f5a68e626",
   "metadata": {},
   "source": [
    "## Function for training the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "c7814feb-531b-4909-b7bd-250527e2bcc2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a function to initialize the weights\n",
    "def weights_init(m):\n",
    "    # Check if the current layer is a Conv2d or ConvTranspose2d\n",
    "    if isinstance(m, nn.Conv2d) or isinstance(m, nn.ConvTranspose2d):\n",
    "        # Apply weights sampled from a normal distribution\n",
    "        torch.nn.init.normal_(m.weight, 0.0, 0.02)\n",
    "    # Check if the current layer is a normalization layer\n",
    "    if isinstance(m, nn.BatchNorm2d):\n",
    "        # Apply weights sampled from a normal distribution\n",
    "        torch.nn.init.normal_(m.weight, 0.0, 0.02)\n",
    "        # Set the bias as zero\n",
    "        torch.nn.init.constant_(m.bias, 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "4fdd9586-a7e7-4a0b-a56f-ff34681f11cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the generators\n",
    "gen_XY = CycleGAN_IR_Generator_DWS(input_channels = 3, initial_channels = 64, output_channels = 3,\n",
    "                              res_blocks = 9, expansion_factor = 3).to(device)\n",
    "gen_YX = CycleGAN_IR_Generator_DWS(input_channels = 3, initial_channels = 64, output_channels = 3,\n",
    "                              res_blocks = 9, expansion_factor = 3).to(device)\n",
    "# Create the optimizer for the generators\n",
    "gen_opt = torch.optim.Adam(list(gen_XY.parameters()) + list(gen_YX.parameters()), lr=LEARNING_RATE, betas=(0.5, 0.999))\n",
    "\n",
    "# Create discriminator X and it's optimizer\n",
    "disc_X = CycleGAN_Discriminator_DWS(3, 64).to(device)\n",
    "disc_X_opt = torch.optim.Adam(disc_X.parameters(), lr=LEARNING_RATE, betas=(0.5, 0.999))\n",
    "# Create discriminator Y and it's optimizer\n",
    "disc_Y = CycleGAN_Discriminator_DWS(3, 64).to(device)\n",
    "disc_Y_opt = torch.optim.Adam(disc_Y.parameters(), lr=LEARNING_RATE, betas=(0.5, 0.999))\n",
    "\n",
    "\n",
    "# Set whether to use a checkpoint and continue from there or start training from scratch\n",
    "load_model = False\n",
    "if load_model == True:\n",
    "    # Load the model weights and optimizer \n",
    "    CycleGAN_dict = torch.load(\"Model Weights/CycleGAN_IR/CycleGAN_IR_epoch100.pth\")\n",
    "    # Apply the weights and optimizer\n",
    "    gen_XY.load_state_dict(CycleGAN_dict['gen_XY'])\n",
    "    gen_YX.load_state_dict(CycleGAN_dict['gen_YX'])\n",
    "    gen_opt.load_state_dict(CycleGAN_dict['gen_opt'])\n",
    "    disc_X.load_state_dict(CycleGAN_dict['disc_X'])\n",
    "    disc_Y.load_state_dict(CycleGAN_dict['disc_Y'])\n",
    "    disc_X_opt.load_state_dict(CycleGAN_dict['disc_X_opt'])\n",
    "    disc_Y_opt.load_state_dict(CycleGAN_dict['disc_Y_opt'])\n",
    "else:\n",
    "    # Apply the weights to the generators and discriminators sampled from a normal distribution\n",
    "    gen_XY = gen_XY.apply(weights_init)\n",
    "    gen_YX = gen_YX.apply(weights_init)\n",
    "    disc_X = disc_X.apply(weights_init)\n",
    "    disc_Y = disc_Y.apply(weights_init)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "fe3ecaea-648e-49b6-9c2c-05b495aa4f2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define loss functions\n",
    "loss_func_adversarial = nn.MSELoss()\n",
    "loss_func_cycle = nn.L1Loss()\n",
    "loss_func_identity = nn.L1Loss()\n",
    "\n",
    "# Define the weights for the loss functions\n",
    "lambda_cycle = 10\n",
    "lambda_identity = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "5971c750-ee0a-4ded-9533-8a80f2273bf6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------------------------------------------\n",
      "        Layer (type)               Output Shape         Param #\n",
      "================================================================\n",
      "            Conv2d-1          [-1, 3, 256, 256]             150\n",
      "            Conv2d-2         [-1, 64, 256, 256]             256\n",
      "  InitialBlock_DWS-3         [-1, 64, 256, 256]               0\n",
      "            Conv2d-4         [-1, 64, 128, 128]             640\n",
      "            Conv2d-5        [-1, 128, 128, 128]           8,320\n",
      "    InstanceNorm2d-6        [-1, 128, 128, 128]               0\n",
      "              ReLU-7        [-1, 128, 128, 128]               0\n",
      "     DownBlock_DWS-8        [-1, 128, 128, 128]               0\n",
      "            Conv2d-9          [-1, 128, 64, 64]           1,280\n",
      "           Conv2d-10          [-1, 256, 64, 64]          33,024\n",
      "   InstanceNorm2d-11          [-1, 256, 64, 64]               0\n",
      "             ReLU-12          [-1, 256, 64, 64]               0\n",
      "    DownBlock_DWS-13          [-1, 256, 64, 64]               0\n",
      "           Conv2d-14          [-1, 512, 64, 64]         131,584\n",
      "   InstanceNorm2d-15          [-1, 512, 64, 64]               0\n",
      "             ReLU-16          [-1, 512, 64, 64]               0\n",
      "           Conv2d-17          [-1, 512, 64, 64]           5,120\n",
      "   InstanceNorm2d-18          [-1, 512, 64, 64]               0\n",
      "             ReLU-19          [-1, 512, 64, 64]               0\n",
      "           Conv2d-20          [-1, 256, 64, 64]         131,328\n",
      "   InstanceNorm2d-21          [-1, 256, 64, 64]               0\n",
      "             ReLU-22          [-1, 256, 64, 64]               0\n",
      "InvertedResidualBlock-23          [-1, 256, 64, 64]               0\n",
      "           Conv2d-24          [-1, 512, 64, 64]         131,584\n",
      "   InstanceNorm2d-25          [-1, 512, 64, 64]               0\n",
      "             ReLU-26          [-1, 512, 64, 64]               0\n",
      "           Conv2d-27          [-1, 512, 64, 64]           5,120\n",
      "   InstanceNorm2d-28          [-1, 512, 64, 64]               0\n",
      "             ReLU-29          [-1, 512, 64, 64]               0\n",
      "           Conv2d-30          [-1, 256, 64, 64]         131,328\n",
      "   InstanceNorm2d-31          [-1, 256, 64, 64]               0\n",
      "             ReLU-32          [-1, 256, 64, 64]               0\n",
      "InvertedResidualBlock-33          [-1, 256, 64, 64]               0\n",
      "           Conv2d-34          [-1, 512, 64, 64]         131,584\n",
      "   InstanceNorm2d-35          [-1, 512, 64, 64]               0\n",
      "             ReLU-36          [-1, 512, 64, 64]               0\n",
      "           Conv2d-37          [-1, 512, 64, 64]           5,120\n",
      "   InstanceNorm2d-38          [-1, 512, 64, 64]               0\n",
      "             ReLU-39          [-1, 512, 64, 64]               0\n",
      "           Conv2d-40          [-1, 256, 64, 64]         131,328\n",
      "   InstanceNorm2d-41          [-1, 256, 64, 64]               0\n",
      "             ReLU-42          [-1, 256, 64, 64]               0\n",
      "InvertedResidualBlock-43          [-1, 256, 64, 64]               0\n",
      "           Conv2d-44          [-1, 512, 64, 64]         131,584\n",
      "   InstanceNorm2d-45          [-1, 512, 64, 64]               0\n",
      "             ReLU-46          [-1, 512, 64, 64]               0\n",
      "           Conv2d-47          [-1, 512, 64, 64]           5,120\n",
      "   InstanceNorm2d-48          [-1, 512, 64, 64]               0\n",
      "             ReLU-49          [-1, 512, 64, 64]               0\n",
      "           Conv2d-50          [-1, 256, 64, 64]         131,328\n",
      "   InstanceNorm2d-51          [-1, 256, 64, 64]               0\n",
      "             ReLU-52          [-1, 256, 64, 64]               0\n",
      "InvertedResidualBlock-53          [-1, 256, 64, 64]               0\n",
      "           Conv2d-54          [-1, 512, 64, 64]         131,584\n",
      "   InstanceNorm2d-55          [-1, 512, 64, 64]               0\n",
      "             ReLU-56          [-1, 512, 64, 64]               0\n",
      "           Conv2d-57          [-1, 512, 64, 64]           5,120\n",
      "   InstanceNorm2d-58          [-1, 512, 64, 64]               0\n",
      "             ReLU-59          [-1, 512, 64, 64]               0\n",
      "           Conv2d-60          [-1, 256, 64, 64]         131,328\n",
      "   InstanceNorm2d-61          [-1, 256, 64, 64]               0\n",
      "             ReLU-62          [-1, 256, 64, 64]               0\n",
      "InvertedResidualBlock-63          [-1, 256, 64, 64]               0\n",
      "           Conv2d-64          [-1, 512, 64, 64]         131,584\n",
      "   InstanceNorm2d-65          [-1, 512, 64, 64]               0\n",
      "             ReLU-66          [-1, 512, 64, 64]               0\n",
      "           Conv2d-67          [-1, 512, 64, 64]           5,120\n",
      "   InstanceNorm2d-68          [-1, 512, 64, 64]               0\n",
      "             ReLU-69          [-1, 512, 64, 64]               0\n",
      "           Conv2d-70          [-1, 256, 64, 64]         131,328\n",
      "   InstanceNorm2d-71          [-1, 256, 64, 64]               0\n",
      "             ReLU-72          [-1, 256, 64, 64]               0\n",
      "InvertedResidualBlock-73          [-1, 256, 64, 64]               0\n",
      "           Conv2d-74          [-1, 512, 64, 64]         131,584\n",
      "   InstanceNorm2d-75          [-1, 512, 64, 64]               0\n",
      "             ReLU-76          [-1, 512, 64, 64]               0\n",
      "           Conv2d-77          [-1, 512, 64, 64]           5,120\n",
      "   InstanceNorm2d-78          [-1, 512, 64, 64]               0\n",
      "             ReLU-79          [-1, 512, 64, 64]               0\n",
      "           Conv2d-80          [-1, 256, 64, 64]         131,328\n",
      "   InstanceNorm2d-81          [-1, 256, 64, 64]               0\n",
      "             ReLU-82          [-1, 256, 64, 64]               0\n",
      "InvertedResidualBlock-83          [-1, 256, 64, 64]               0\n",
      "           Conv2d-84          [-1, 512, 64, 64]         131,584\n",
      "   InstanceNorm2d-85          [-1, 512, 64, 64]               0\n",
      "             ReLU-86          [-1, 512, 64, 64]               0\n",
      "           Conv2d-87          [-1, 512, 64, 64]           5,120\n",
      "   InstanceNorm2d-88          [-1, 512, 64, 64]               0\n",
      "             ReLU-89          [-1, 512, 64, 64]               0\n",
      "           Conv2d-90          [-1, 256, 64, 64]         131,328\n",
      "   InstanceNorm2d-91          [-1, 256, 64, 64]               0\n",
      "             ReLU-92          [-1, 256, 64, 64]               0\n",
      "InvertedResidualBlock-93          [-1, 256, 64, 64]               0\n",
      "           Conv2d-94          [-1, 512, 64, 64]         131,584\n",
      "   InstanceNorm2d-95          [-1, 512, 64, 64]               0\n",
      "             ReLU-96          [-1, 512, 64, 64]               0\n",
      "           Conv2d-97          [-1, 512, 64, 64]           5,120\n",
      "   InstanceNorm2d-98          [-1, 512, 64, 64]               0\n",
      "             ReLU-99          [-1, 512, 64, 64]               0\n",
      "          Conv2d-100          [-1, 256, 64, 64]         131,328\n",
      "  InstanceNorm2d-101          [-1, 256, 64, 64]               0\n",
      "            ReLU-102          [-1, 256, 64, 64]               0\n",
      "InvertedResidualBlock-103          [-1, 256, 64, 64]               0\n",
      " ConvTranspose2d-104        [-1, 256, 128, 128]           2,560\n",
      "          Conv2d-105        [-1, 128, 128, 128]          32,896\n",
      "  InstanceNorm2d-106        [-1, 128, 128, 128]               0\n",
      "            ReLU-107        [-1, 128, 128, 128]               0\n",
      "     UpBlock_DWS-108        [-1, 128, 128, 128]               0\n",
      " ConvTranspose2d-109        [-1, 128, 256, 256]           1,280\n",
      "          Conv2d-110         [-1, 64, 256, 256]           8,256\n",
      "  InstanceNorm2d-111         [-1, 64, 256, 256]               0\n",
      "            ReLU-112         [-1, 64, 256, 256]               0\n",
      "     UpBlock_DWS-113         [-1, 64, 256, 256]               0\n",
      "          Conv2d-114         [-1, 64, 256, 256]           3,200\n",
      "          Conv2d-115          [-1, 3, 256, 256]             195\n",
      "InitialBlock_DWS-116          [-1, 3, 256, 256]               0\n",
      "            Tanh-117          [-1, 3, 256, 256]               0\n",
      "================================================================\n",
      "Total params: 2,504,345\n",
      "Trainable params: 2,504,345\n",
      "Non-trainable params: 0\n",
      "----------------------------------------------------------------\n",
      "Input size (MB): 0.75\n",
      "Forward/backward pass size (MB): 1650.00\n",
      "Params size (MB): 9.55\n",
      "Estimated Total Size (MB): 1660.30\n",
      "----------------------------------------------------------------\n",
      "----------------------------------------------------------------\n",
      "        Layer (type)               Output Shape         Param #\n",
      "================================================================\n",
      "            Conv2d-1          [-1, 3, 256, 256]             150\n",
      "            Conv2d-2         [-1, 64, 256, 256]             256\n",
      "  InitialBlock_DWS-3         [-1, 64, 256, 256]               0\n",
      "            Conv2d-4         [-1, 64, 128, 128]             640\n",
      "            Conv2d-5        [-1, 128, 128, 128]           8,320\n",
      "    InstanceNorm2d-6        [-1, 128, 128, 128]               0\n",
      "              ReLU-7        [-1, 128, 128, 128]               0\n",
      "     DownBlock_DWS-8        [-1, 128, 128, 128]               0\n",
      "            Conv2d-9          [-1, 128, 64, 64]           1,280\n",
      "           Conv2d-10          [-1, 256, 64, 64]          33,024\n",
      "   InstanceNorm2d-11          [-1, 256, 64, 64]               0\n",
      "             ReLU-12          [-1, 256, 64, 64]               0\n",
      "    DownBlock_DWS-13          [-1, 256, 64, 64]               0\n",
      "           Conv2d-14          [-1, 512, 64, 64]         131,584\n",
      "   InstanceNorm2d-15          [-1, 512, 64, 64]               0\n",
      "             ReLU-16          [-1, 512, 64, 64]               0\n",
      "           Conv2d-17          [-1, 512, 64, 64]           5,120\n",
      "   InstanceNorm2d-18          [-1, 512, 64, 64]               0\n",
      "             ReLU-19          [-1, 512, 64, 64]               0\n",
      "           Conv2d-20          [-1, 256, 64, 64]         131,328\n",
      "   InstanceNorm2d-21          [-1, 256, 64, 64]               0\n",
      "             ReLU-22          [-1, 256, 64, 64]               0\n",
      "InvertedResidualBlock-23          [-1, 256, 64, 64]               0\n",
      "           Conv2d-24          [-1, 512, 64, 64]         131,584\n",
      "   InstanceNorm2d-25          [-1, 512, 64, 64]               0\n",
      "             ReLU-26          [-1, 512, 64, 64]               0\n",
      "           Conv2d-27          [-1, 512, 64, 64]           5,120\n",
      "   InstanceNorm2d-28          [-1, 512, 64, 64]               0\n",
      "             ReLU-29          [-1, 512, 64, 64]               0\n",
      "           Conv2d-30          [-1, 256, 64, 64]         131,328\n",
      "   InstanceNorm2d-31          [-1, 256, 64, 64]               0\n",
      "             ReLU-32          [-1, 256, 64, 64]               0\n",
      "InvertedResidualBlock-33          [-1, 256, 64, 64]               0\n",
      "           Conv2d-34          [-1, 512, 64, 64]         131,584\n",
      "   InstanceNorm2d-35          [-1, 512, 64, 64]               0\n",
      "             ReLU-36          [-1, 512, 64, 64]               0\n",
      "           Conv2d-37          [-1, 512, 64, 64]           5,120\n",
      "   InstanceNorm2d-38          [-1, 512, 64, 64]               0\n",
      "             ReLU-39          [-1, 512, 64, 64]               0\n",
      "           Conv2d-40          [-1, 256, 64, 64]         131,328\n",
      "   InstanceNorm2d-41          [-1, 256, 64, 64]               0\n",
      "             ReLU-42          [-1, 256, 64, 64]               0\n",
      "InvertedResidualBlock-43          [-1, 256, 64, 64]               0\n",
      "           Conv2d-44          [-1, 512, 64, 64]         131,584\n",
      "   InstanceNorm2d-45          [-1, 512, 64, 64]               0\n",
      "             ReLU-46          [-1, 512, 64, 64]               0\n",
      "           Conv2d-47          [-1, 512, 64, 64]           5,120\n",
      "   InstanceNorm2d-48          [-1, 512, 64, 64]               0\n",
      "             ReLU-49          [-1, 512, 64, 64]               0\n",
      "           Conv2d-50          [-1, 256, 64, 64]         131,328\n",
      "   InstanceNorm2d-51          [-1, 256, 64, 64]               0\n",
      "             ReLU-52          [-1, 256, 64, 64]               0\n",
      "InvertedResidualBlock-53          [-1, 256, 64, 64]               0\n",
      "           Conv2d-54          [-1, 512, 64, 64]         131,584\n",
      "   InstanceNorm2d-55          [-1, 512, 64, 64]               0\n",
      "             ReLU-56          [-1, 512, 64, 64]               0\n",
      "           Conv2d-57          [-1, 512, 64, 64]           5,120\n",
      "   InstanceNorm2d-58          [-1, 512, 64, 64]               0\n",
      "             ReLU-59          [-1, 512, 64, 64]               0\n",
      "           Conv2d-60          [-1, 256, 64, 64]         131,328\n",
      "   InstanceNorm2d-61          [-1, 256, 64, 64]               0\n",
      "             ReLU-62          [-1, 256, 64, 64]               0\n",
      "InvertedResidualBlock-63          [-1, 256, 64, 64]               0\n",
      "           Conv2d-64          [-1, 512, 64, 64]         131,584\n",
      "   InstanceNorm2d-65          [-1, 512, 64, 64]               0\n",
      "             ReLU-66          [-1, 512, 64, 64]               0\n",
      "           Conv2d-67          [-1, 512, 64, 64]           5,120\n",
      "   InstanceNorm2d-68          [-1, 512, 64, 64]               0\n",
      "             ReLU-69          [-1, 512, 64, 64]               0\n",
      "           Conv2d-70          [-1, 256, 64, 64]         131,328\n",
      "   InstanceNorm2d-71          [-1, 256, 64, 64]               0\n",
      "             ReLU-72          [-1, 256, 64, 64]               0\n",
      "InvertedResidualBlock-73          [-1, 256, 64, 64]               0\n",
      "           Conv2d-74          [-1, 512, 64, 64]         131,584\n",
      "   InstanceNorm2d-75          [-1, 512, 64, 64]               0\n",
      "             ReLU-76          [-1, 512, 64, 64]               0\n",
      "           Conv2d-77          [-1, 512, 64, 64]           5,120\n",
      "   InstanceNorm2d-78          [-1, 512, 64, 64]               0\n",
      "             ReLU-79          [-1, 512, 64, 64]               0\n",
      "           Conv2d-80          [-1, 256, 64, 64]         131,328\n",
      "   InstanceNorm2d-81          [-1, 256, 64, 64]               0\n",
      "             ReLU-82          [-1, 256, 64, 64]               0\n",
      "InvertedResidualBlock-83          [-1, 256, 64, 64]               0\n",
      "           Conv2d-84          [-1, 512, 64, 64]         131,584\n",
      "   InstanceNorm2d-85          [-1, 512, 64, 64]               0\n",
      "             ReLU-86          [-1, 512, 64, 64]               0\n",
      "           Conv2d-87          [-1, 512, 64, 64]           5,120\n",
      "   InstanceNorm2d-88          [-1, 512, 64, 64]               0\n",
      "             ReLU-89          [-1, 512, 64, 64]               0\n",
      "           Conv2d-90          [-1, 256, 64, 64]         131,328\n",
      "   InstanceNorm2d-91          [-1, 256, 64, 64]               0\n",
      "             ReLU-92          [-1, 256, 64, 64]               0\n",
      "InvertedResidualBlock-93          [-1, 256, 64, 64]               0\n",
      "           Conv2d-94          [-1, 512, 64, 64]         131,584\n",
      "   InstanceNorm2d-95          [-1, 512, 64, 64]               0\n",
      "             ReLU-96          [-1, 512, 64, 64]               0\n",
      "           Conv2d-97          [-1, 512, 64, 64]           5,120\n",
      "   InstanceNorm2d-98          [-1, 512, 64, 64]               0\n",
      "             ReLU-99          [-1, 512, 64, 64]               0\n",
      "          Conv2d-100          [-1, 256, 64, 64]         131,328\n",
      "  InstanceNorm2d-101          [-1, 256, 64, 64]               0\n",
      "            ReLU-102          [-1, 256, 64, 64]               0\n",
      "InvertedResidualBlock-103          [-1, 256, 64, 64]               0\n",
      " ConvTranspose2d-104        [-1, 256, 128, 128]           2,560\n",
      "          Conv2d-105        [-1, 128, 128, 128]          32,896\n",
      "  InstanceNorm2d-106        [-1, 128, 128, 128]               0\n",
      "            ReLU-107        [-1, 128, 128, 128]               0\n",
      "     UpBlock_DWS-108        [-1, 128, 128, 128]               0\n",
      " ConvTranspose2d-109        [-1, 128, 256, 256]           1,280\n",
      "          Conv2d-110         [-1, 64, 256, 256]           8,256\n",
      "  InstanceNorm2d-111         [-1, 64, 256, 256]               0\n",
      "            ReLU-112         [-1, 64, 256, 256]               0\n",
      "     UpBlock_DWS-113         [-1, 64, 256, 256]               0\n",
      "          Conv2d-114         [-1, 64, 256, 256]           3,200\n",
      "          Conv2d-115          [-1, 3, 256, 256]             195\n",
      "InitialBlock_DWS-116          [-1, 3, 256, 256]               0\n",
      "            Tanh-117          [-1, 3, 256, 256]               0\n",
      "================================================================\n",
      "Total params: 2,504,345\n",
      "Trainable params: 2,504,345\n",
      "Non-trainable params: 0\n",
      "----------------------------------------------------------------\n",
      "Input size (MB): 0.75\n",
      "Forward/backward pass size (MB): 1650.00\n",
      "Params size (MB): 9.55\n",
      "Estimated Total Size (MB): 1660.30\n",
      "----------------------------------------------------------------\n",
      "----------------------------------------------------------------\n",
      "        Layer (type)               Output Shape         Param #\n",
      "================================================================\n",
      "            Conv2d-1          [-1, 3, 256, 256]             150\n",
      "            Conv2d-2         [-1, 64, 256, 256]             256\n",
      "  InitialBlock_DWS-3         [-1, 64, 256, 256]               0\n",
      "            Conv2d-4         [-1, 64, 128, 128]           1,088\n",
      "            Conv2d-5         [-1, 64, 128, 128]           4,160\n",
      "    InstanceNorm2d-6         [-1, 64, 128, 128]               0\n",
      "         LeakyReLU-7         [-1, 64, 128, 128]               0\n",
      "     DownBlock_DWS-8         [-1, 64, 128, 128]               0\n",
      "            Conv2d-9           [-1, 64, 64, 64]           1,088\n",
      "           Conv2d-10          [-1, 128, 64, 64]           8,320\n",
      "   InstanceNorm2d-11          [-1, 128, 64, 64]               0\n",
      "        LeakyReLU-12          [-1, 128, 64, 64]               0\n",
      "    DownBlock_DWS-13          [-1, 128, 64, 64]               0\n",
      "           Conv2d-14          [-1, 128, 32, 32]           2,176\n",
      "           Conv2d-15          [-1, 256, 32, 32]          33,024\n",
      "   InstanceNorm2d-16          [-1, 256, 32, 32]               0\n",
      "        LeakyReLU-17          [-1, 256, 32, 32]               0\n",
      "    DownBlock_DWS-18          [-1, 256, 32, 32]               0\n",
      "           Conv2d-19          [-1, 256, 31, 31]           4,352\n",
      "           Conv2d-20          [-1, 512, 31, 31]         131,584\n",
      "   InstanceNorm2d-21          [-1, 512, 31, 31]               0\n",
      "        LeakyReLU-22          [-1, 512, 31, 31]               0\n",
      "    DownBlock_DWS-23          [-1, 512, 31, 31]               0\n",
      "           Conv2d-24          [-1, 512, 30, 30]           8,704\n",
      "           Conv2d-25            [-1, 1, 30, 30]             513\n",
      " InitialBlock_DWS-26            [-1, 1, 30, 30]               0\n",
      "================================================================\n",
      "Total params: 195,415\n",
      "Trainable params: 195,415\n",
      "Non-trainable params: 0\n",
      "----------------------------------------------------------------\n",
      "Input size (MB): 0.75\n",
      "Forward/backward pass size (MB): 152.92\n",
      "Params size (MB): 0.75\n",
      "Estimated Total Size (MB): 154.42\n",
      "----------------------------------------------------------------\n",
      "----------------------------------------------------------------\n",
      "        Layer (type)               Output Shape         Param #\n",
      "================================================================\n",
      "            Conv2d-1          [-1, 3, 256, 256]             150\n",
      "            Conv2d-2         [-1, 64, 256, 256]             256\n",
      "  InitialBlock_DWS-3         [-1, 64, 256, 256]               0\n",
      "            Conv2d-4         [-1, 64, 128, 128]           1,088\n",
      "            Conv2d-5         [-1, 64, 128, 128]           4,160\n",
      "    InstanceNorm2d-6         [-1, 64, 128, 128]               0\n",
      "         LeakyReLU-7         [-1, 64, 128, 128]               0\n",
      "     DownBlock_DWS-8         [-1, 64, 128, 128]               0\n",
      "            Conv2d-9           [-1, 64, 64, 64]           1,088\n",
      "           Conv2d-10          [-1, 128, 64, 64]           8,320\n",
      "   InstanceNorm2d-11          [-1, 128, 64, 64]               0\n",
      "        LeakyReLU-12          [-1, 128, 64, 64]               0\n",
      "    DownBlock_DWS-13          [-1, 128, 64, 64]               0\n",
      "           Conv2d-14          [-1, 128, 32, 32]           2,176\n",
      "           Conv2d-15          [-1, 256, 32, 32]          33,024\n",
      "   InstanceNorm2d-16          [-1, 256, 32, 32]               0\n",
      "        LeakyReLU-17          [-1, 256, 32, 32]               0\n",
      "    DownBlock_DWS-18          [-1, 256, 32, 32]               0\n",
      "           Conv2d-19          [-1, 256, 31, 31]           4,352\n",
      "           Conv2d-20          [-1, 512, 31, 31]         131,584\n",
      "   InstanceNorm2d-21          [-1, 512, 31, 31]               0\n",
      "        LeakyReLU-22          [-1, 512, 31, 31]               0\n",
      "    DownBlock_DWS-23          [-1, 512, 31, 31]               0\n",
      "           Conv2d-24          [-1, 512, 30, 30]           8,704\n",
      "           Conv2d-25            [-1, 1, 30, 30]             513\n",
      " InitialBlock_DWS-26            [-1, 1, 30, 30]               0\n",
      "================================================================\n",
      "Total params: 195,415\n",
      "Trainable params: 195,415\n",
      "Non-trainable params: 0\n",
      "----------------------------------------------------------------\n",
      "Input size (MB): 0.75\n",
      "Forward/backward pass size (MB): 152.92\n",
      "Params size (MB): 0.75\n",
      "Estimated Total Size (MB): 154.42\n",
      "----------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "# Print the summary of the Generator XY\n",
    "#summary(gen_XY, (3, 256, 256))\n",
    "\n",
    "# Print the summary of the Generator YX\n",
    "#summary(gen_YX, (3, 256, 256))\n",
    "\n",
    "# Print the summary of the Discriminator X\n",
    "#summary(disc_X, (3, 256, 256))\n",
    "\n",
    "# Print the summary of the Discriminator Y\n",
    "#summary(disc_Y, (3, 256, 256))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "0eea36ab-306e-444a-a78c-23318b3a6251",
   "metadata": {},
   "outputs": [],
   "source": [
    "visl = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "676f31c8-cab5-45ca-9c74-3c7b1fbe379b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the function to train the CycleGAN\n",
    "def train_CycleGAN_IR_DWS():\n",
    "    \"\"\"\n",
    "    Function to train CycleGAN with inverted residual blocks instead of residual blocks. \n",
    "    Doesn't take any inputs.\n",
    "    \n",
    "    Outputs: Saves the model to disk\n",
    "             Displays the real and generated images after every few steps (If chosen)\n",
    "             Prints the mean generator and discriminator losses after every epoch\n",
    "    \"\"\"\n",
    "    \n",
    "    # Define variables to keep track of mean generator and discriminator losses\n",
    "    loss_generator_mean = 0\n",
    "    loss_discriminator_mean = 0\n",
    "    # Create a counter that to keep track of how many images have been processed\n",
    "    # and to display real and generated images after certain images have been processed\n",
    "    disp = 1\n",
    "    \n",
    "    # Keep training for a certain number of epochs\n",
    "    for epoch in range(1, NUM_EPOCHS):\n",
    "        # Use the tqdm to load the data\n",
    "        for real_img_X, real_img_Y in tqdm(dataloader):\n",
    "            # Move the images to the device\n",
    "            real_img_X = real_img_X.to(device)\n",
    "            real_img_Y = real_img_Y.to(device)\n",
    "\n",
    "            # UPDATE THE GENS\n",
    "            # Set the gradients to zero\n",
    "            gen_opt.zero_grad()\n",
    "            # Compute the generator loss\n",
    "            loss_gen = gen_loss_complete(real_img_X, real_img_Y,\n",
    "                                         gen_XY, gen_YX, disc_X, disc_Y,  \n",
    "                                         loss_func_adversarial, loss_func_cycle, loss_func_identity, \n",
    "                                         lambda_cycle, lambda_identity, \n",
    "                                         add_identity_loss = True)\n",
    "            # Perform backpropogation for the generator\n",
    "            loss_gen.backward()\n",
    "            # Update the generators by taking an optimization step\n",
    "            gen_opt.step()\n",
    "            \n",
    "            # UPDATE THE DISCS\n",
    "            # Update disc_X \n",
    "            # Set the gradients to zero\n",
    "            disc_X_opt.zero_grad()\n",
    "            # Generate a fake image in domain X\n",
    "            with torch.no_grad():\n",
    "                fake_img_X = gen_YX(real_img_Y)\n",
    "            # Compute the loss\n",
    "            loss_disc_X = disc_loss(real_img_X, fake_img_X.detach(), disc_X, loss_func_adversarial)\n",
    "            # Perform backpropogation for disc_X\n",
    "            loss_disc_X.backward(retain_graph=True)\n",
    "            # Update disc_X opt\n",
    "            disc_X_opt.step() \n",
    "            \n",
    "            # Update disc_Y\n",
    "            # Set the gradients to zero\n",
    "            disc_Y_opt.zero_grad() \n",
    "            # Generate a fake image in domain Y\n",
    "            with torch.no_grad():\n",
    "                fake_img_Y = gen_XY(real_img_X)\n",
    "            # Compute the loss\n",
    "            loss_disc_Y = disc_loss(real_img_Y, fake_img_Y.detach(), disc_Y, loss_func_adversarial)\n",
    "            # Perform backpropogation for disc_Y\n",
    "            loss_disc_Y.backward(retain_graph=True)\n",
    "            # Update disc_Y opt\n",
    "            disc_Y_opt.step() \n",
    "            \n",
    "            # Compute the mean generator and discriminator loss\n",
    "            loss_generator_mean = loss_gen.item()/disp\n",
    "            loss_discriminator_mean = ( loss_disc_X + loss_disc_Y ).item() / disp\n",
    "            \n",
    "            # Check if the images are to be visualized\n",
    "            if visl == True:\n",
    "                # Code for visualization\n",
    "                if disp % 10 == 0:\n",
    "                    # Visualize the images\n",
    "                    # The transform applied to the images 0.5 and divides by 0.5\n",
    "                    # To un-transform we have to add 1 and divide by 2\n",
    "                    visualize_images( (real_img_X +1 ) / 2, (real_img_Y + 1) / 2, (fake_img_X + 1) / 2, (fake_img_Y + 1) / 2)\n",
    "                    # Display the mean generator and discriminator losses\n",
    "                    print(f\"Display step is: {disp}\\n\"\n",
    "                      f\"The average discriminator loss is: {loss_discriminator_mean}\\n\"\n",
    "                      f\"The average generator loss is: {loss_generator_mean}\\n\")\n",
    "            # Increase the display counter\n",
    "            disp+= 1\n",
    "        # For loop for loading data using tqdm ends here\n",
    "        \n",
    "        # Print the generator and discriminator loss at each epoch\n",
    "        print(f\"Epoch is: {epoch}\\n\"\n",
    "              f\"The average discriminator loss is: {loss_discriminator_mean}\\n\"\n",
    "              f\"The average generator loss is: {loss_generator_mean}\\n\")\n",
    "            \n",
    "        # Save the model after every 10 epochs\n",
    "        if epoch % 10 == 0:\n",
    "            # Define a save path\n",
    "            save_path = \"Model Weights/CycleGAN_IR_DWS/\"\n",
    "            # Create a variable storing the current checkpoint name\n",
    "            name_model_epoch = f\"CycleGAN_IR_DWS_epoch{epoch}.pth\"\n",
    "            # Create a directory if it does not exist\n",
    "            if not os.path.exists(save_path):\n",
    "                os.makedirs(save_path, exist_ok = True)\n",
    "            # Save the model to this directory\n",
    "            torch.save({\n",
    "                    'gen_XY': gen_XY.state_dict(),\n",
    "                    'gen_YX': gen_YX.state_dict(),\n",
    "                    'gen_opt': gen_opt.state_dict(),\n",
    "                    'disc_X': disc_X.state_dict(),\n",
    "                    'disc_Y': disc_Y.state_dict(),\n",
    "                    'disc_X_opt':disc_X_opt.state_dict(),\n",
    "                    'disc_Y_opt':disc_Y_opt.state_dict()\n",
    "                }, save_path + name_model_epoch)\n",
    "# End of function definition        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "e06619ef-199a-4ac5-92c9-948bc7f26d08",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/1067 [00:03<?, ?it/s]\n"
     ]
    },
    {
     "ename": "OutOfMemoryError",
     "evalue": "CUDA out of memory. Tried to allocate 20.00 MiB (GPU 0; 6.00 GiB total capacity; 5.25 GiB already allocated; 0 bytes free; 5.34 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mOutOfMemoryError\u001b[0m                          Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_13992\\1019572572.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;31m# Train the CycleGAN\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[0mtrain_CycleGAN_IR_DWS\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_13992\\1704284708.py\u001b[0m in \u001b[0;36mtrain_CycleGAN_IR_DWS\u001b[1;34m()\u001b[0m\n\u001b[0;32m     56\u001b[0m             \u001b[0mgen_opt\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     57\u001b[0m             \u001b[1;31m# Compute the generator loss\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 58\u001b[1;33m             loss_gen = gen_loss_complete(real_img_X, real_img_Y,\n\u001b[0m\u001b[0;32m     59\u001b[0m                                          \u001b[0mgen_XY\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mgen_YX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdisc_X\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdisc_Y\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     60\u001b[0m                                          \u001b[0mloss_func_adversarial\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mloss_func_cycle\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mloss_func_identity\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Desktop\\University\\1.Studies\\Semester 6\\Bachelor Thesis CSAI\\CycleGAN\\Models\\Loss_functions.py\u001b[0m in \u001b[0;36mgen_loss_complete\u001b[1;34m(real_img_X, real_img_Y, gen_XY, gen_YX, disc_X, disc_Y, loss_func_adversarial, loss_func_cycle, loss_func_identity, lambda_cycle, lambda_identity, add_identity_loss)\u001b[0m\n\u001b[0;32m    202\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0madd_identity_loss\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;32mTrue\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    203\u001b[0m         \u001b[1;31m# Compute the identity loss\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 204\u001b[1;33m         \u001b[0mgen_loss_iden\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mgen_loss_identity\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mreal_img_X\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mgen_YX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mloss_func_identity\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mgen_loss_identity\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mreal_img_Y\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mgen_XY\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mloss_func_identity\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    205\u001b[0m         \u001b[1;31m# Add to the generator loss\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    206\u001b[0m         \u001b[0mgen_loss\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mgen_loss\u001b[0m \u001b[1;33m+\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mgen_loss_iden\u001b[0m \u001b[1;33m*\u001b[0m \u001b[0mlambda_identity\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Desktop\\University\\1.Studies\\Semester 6\\Bachelor Thesis CSAI\\CycleGAN\\Models\\Loss_functions.py\u001b[0m in \u001b[0;36mgen_loss_identity\u001b[1;34m(real_img_X, genYX, loss_func)\u001b[0m\n\u001b[0;32m    160\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    161\u001b[0m     \u001b[1;31m# Compute the and return the identity loss\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 162\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[0mloss_func\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mreal_img_X\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mgenYX\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mreal_img_X\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    163\u001b[0m \u001b[1;31m###\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    164\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\Anaconda3\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   1192\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[0;32m   1193\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[1;32m-> 1194\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1195\u001b[0m         \u001b[1;31m# Do not call functions when jit is used\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1196\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Desktop\\University\\1.Studies\\Semester 6\\Bachelor Thesis CSAI\\CycleGAN\\Models\\Generators.py\u001b[0m in \u001b[0;36mforward\u001b[1;34m(self, x)\u001b[0m\n\u001b[0;32m    266\u001b[0m         \u001b[0mx\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdown_dws\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    267\u001b[0m         \u001b[1;31m# residual blocks\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 268\u001b[1;33m         \u001b[0mx\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mres_dws\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    269\u001b[0m         \u001b[1;31m# Upsampling layers\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    270\u001b[0m         \u001b[0mx\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mup_dws\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\Anaconda3\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   1192\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[0;32m   1193\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[1;32m-> 1194\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1195\u001b[0m         \u001b[1;31m# Do not call functions when jit is used\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1196\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\Anaconda3\\lib\\site-packages\\torch\\nn\\modules\\container.py\u001b[0m in \u001b[0;36mforward\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m    202\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minput\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    203\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mmodule\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 204\u001b[1;33m             \u001b[0minput\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmodule\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    205\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0minput\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    206\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\Anaconda3\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   1192\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[0;32m   1193\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[1;32m-> 1194\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1195\u001b[0m         \u001b[1;31m# Do not call functions when jit is used\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1196\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Desktop\\University\\1.Studies\\Semester 6\\Bachelor Thesis CSAI\\CycleGAN\\Models\\Blocks.py\u001b[0m in \u001b[0;36mforward\u001b[1;34m(self, x)\u001b[0m\n\u001b[0;32m    209\u001b[0m         \u001b[0morig_x\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mx\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mclone\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    210\u001b[0m         \u001b[1;31m# Apply the expansion\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 211\u001b[1;33m         \u001b[0mx\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mexpansion\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    212\u001b[0m         \u001b[1;31m# Apply the depthwise\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    213\u001b[0m         \u001b[0mx\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdepthwise\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\Anaconda3\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   1192\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[0;32m   1193\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[1;32m-> 1194\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1195\u001b[0m         \u001b[1;31m# Do not call functions when jit is used\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1196\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\Anaconda3\\lib\\site-packages\\torch\\nn\\modules\\container.py\u001b[0m in \u001b[0;36mforward\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m    202\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minput\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    203\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mmodule\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 204\u001b[1;33m             \u001b[0minput\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmodule\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    205\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0minput\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    206\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\Anaconda3\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   1192\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[0;32m   1193\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[1;32m-> 1194\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1195\u001b[0m         \u001b[1;31m# Do not call functions when jit is used\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1196\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\Anaconda3\\lib\\site-packages\\torch\\nn\\modules\\instancenorm.py\u001b[0m in \u001b[0;36mforward\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m     72\u001b[0m             \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_handle_no_batch_input\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     73\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 74\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_apply_instance_norm\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     75\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     76\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\Anaconda3\\lib\\site-packages\\torch\\nn\\modules\\instancenorm.py\u001b[0m in \u001b[0;36m_apply_instance_norm\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m     32\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     33\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_apply_instance_norm\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minput\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 34\u001b[1;33m         return F.instance_norm(\n\u001b[0m\u001b[0;32m     35\u001b[0m             \u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrunning_mean\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrunning_var\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mweight\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbias\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     36\u001b[0m             self.training or not self.track_running_stats, self.momentum, self.eps)\n",
      "\u001b[1;32mD:\\Anaconda3\\lib\\site-packages\\torch\\nn\\functional.py\u001b[0m in \u001b[0;36minstance_norm\u001b[1;34m(input, running_mean, running_var, weight, bias, use_input_stats, momentum, eps)\u001b[0m\n\u001b[0;32m   2493\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0muse_input_stats\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2494\u001b[0m         \u001b[0m_verify_spatial_size\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msize\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2495\u001b[1;33m     return torch.instance_norm(\n\u001b[0m\u001b[0;32m   2496\u001b[0m         \u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mweight\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbias\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mrunning_mean\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mrunning_var\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0muse_input_stats\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmomentum\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0meps\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbackends\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcudnn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0menabled\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2497\u001b[0m     )\n",
      "\u001b[1;31mOutOfMemoryError\u001b[0m: CUDA out of memory. Tried to allocate 20.00 MiB (GPU 0; 6.00 GiB total capacity; 5.25 GiB already allocated; 0 bytes free; 5.34 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF"
     ]
    }
   ],
   "source": [
    "# Train the CycleGAN\n",
    "train_CycleGAN_IR_DWS()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "13acc68a-bf6e-42ab-8b12-f9c426e85113",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[NbConvertApp] Converting notebook train_CycleGAN_IR_DWS.ipynb to script\n",
      "[NbConvertApp] Writing 10398 bytes to train_CycleGAN_IR_DWS.py\n"
     ]
    }
   ],
   "source": [
    "!jupyter nbconvert --to script --no-prompt train_CycleGAN_IR_DWS.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5306f110-00fe-4c24-9128-582970842544",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
